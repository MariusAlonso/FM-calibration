{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b614e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malonso/miniconda3/envs/mantisenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/malonso/miniconda3/envs/mantisenv/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cab78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [05:18<00:00, 78.38it/s]\n",
      "100%|██████████| 25000/25000 [05:11<00:00, 80.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy using GPT-2 embeddings: 89.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malonso/miniconda3/envs/mantisenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can use other variants like gpt2-medium, gpt2-large, gpt2-xl if needed\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Reuse eos_token for padding\n",
    "\n",
    "\n",
    "# Move model to GPU (if available)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Function to get GPT-2 embeddings\n",
    "def get_gpt2_embeddings(texts):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts):\n",
    "            # Tokenize the input text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            \n",
    "            # Get the model's hidden states (embeddings)\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Get the last hidden state (representing the token embeddings)\n",
    "            hidden_states = outputs.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
    "            \n",
    "            # We can use the embedding of the [CLS] token (usually the first token) or average all token embeddings\n",
    "            # Use the mean of all token embeddings (average pooling)\n",
    "            embeddings = hidden_states.mean(dim=1).cpu().numpy()  # Shape: [batch_size, hidden_size]\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Get GPT-2 embeddings for training and test set\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "test_texts = dataset[\"test\"][\"text\"]\n",
    "\n",
    "# Get embeddings for train and test texts\n",
    "train_embeddings = get_gpt2_embeddings(train_texts)\n",
    "test_embeddings = get_gpt2_embeddings(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df250714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89364"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68f65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Use Logistic Regression to classify sentiment (0 = negative, 1 = positive)\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(train_embeddings, dataset[\"train\"][\"label\"])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(test_embeddings)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(dataset[\"test\"][\"label\"], predictions)\n",
    "print(f\"Classification accuracy using GPT-2 embeddings: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d5543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malonso/miniconda3/envs/mantisenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 92.54it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 92.66it/s]\n",
      " 14%|█▍        | 1/7 [00:25<02:30, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: ag_news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 97.96it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.17it/s]\n",
      " 29%|██▊       | 2/7 [00:48<01:59, 23.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: yelp_polarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 95.51it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 95.54it/s]\n",
      " 43%|████▎     | 3/7 [01:12<01:35, 24.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: dbpedia_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 97.63it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.56it/s]\n",
      " 57%|█████▋    | 4/7 [01:35<01:11, 23.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: amazon_polarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 97.42it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.42it/s]\n",
      " 71%|███████▏  | 5/7 [01:59<00:47, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 98.82it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.86it/s]\n",
      " 86%|████████▌ | 6/7 [02:27<00:25, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: rotten_tomatoes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 98.57it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.60it/s]\n",
      "100%|██████████| 7/7 [02:49<00:00, 24.24s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 103.76it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 104.09it/s]\n",
      " 14%|█▍        | 1/7 [00:23<02:18, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: ag_news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 111.67it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 112.43it/s]\n",
      " 29%|██▊       | 2/7 [00:44<01:49, 21.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: yelp_polarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 108.19it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 108.64it/s]\n",
      " 43%|████▎     | 3/7 [01:05<01:26, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: dbpedia_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 111.36it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 111.30it/s]\n",
      " 57%|█████▋    | 4/7 [01:26<01:04, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: amazon_polarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 110.73it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 110.59it/s]\n",
      " 71%|███████▏  | 5/7 [01:49<00:43, 21.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 113.28it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 113.16it/s]\n",
      " 86%|████████▌ | 6/7 [02:10<00:21, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: rotten_tomatoes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 112.77it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 112.85it/s]\n",
      "100%|██████████| 7/7 [02:29<00:00, 21.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Choose model type: 'bert-base-uncased' or 'gpt2'\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# Extract embeddings using mean pooling\n",
    "def get_embeddings(texts, tokenizer, model, max_len=256):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm.tqdm(texts):\n",
    "            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_len).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            pooled = last_hidden.mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(pooled)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# Extract and save embeddings for multiple datasets\n",
    "def process_datasets(model_name, output_prefix, num_samples=1000):\n",
    "    tokenizer, model = load_model_and_tokenizer(model_name)\n",
    "    train_data, test_data = {}, {}\n",
    "\n",
    "    for task in tqdm.tqdm(classification_tasks):\n",
    "        print(f\"\\nProcessing dataset: {task}\")\n",
    "        try:\n",
    "            dataset = load_dataset(task)\n",
    "            if \"train\" not in dataset or \"test\" not in dataset:\n",
    "                print(f\"Skipping {task} due to missing train/test split.\")\n",
    "                continue\n",
    "\n",
    "            # Sample data (limit size for faster processing)\n",
    "            train_indices = (\n",
    "                np.arange(len(dataset[\"train\"])) if len(dataset[\"train\"]) < num_samples\n",
    "                else np.random.permutation(len(dataset[\"train\"]))[:num_samples]\n",
    "            )\n",
    "            train = dataset[\"train\"].select(train_indices)\n",
    "            test_indices = (\n",
    "                np.arange(len(dataset[\"test\"])) if len(dataset[\"test\"]) < num_samples\n",
    "                else np.random.permutation(len(dataset[\"test\"]))[:num_samples]\n",
    "            )\n",
    "            test = dataset[\"test\"].select(test_indices)\n",
    "\n",
    "            # Attempt to use standard field names\n",
    "            text_field = \"text\"\n",
    "            for field in [\"sentence\", \"content\", \"question\", \"review\", \"comment\"]:\n",
    "                if field in train.column_names:\n",
    "                    text_field = field\n",
    "                    break\n",
    "\n",
    "            train_emb = get_embeddings(train[text_field], tokenizer, model)\n",
    "            test_emb = get_embeddings(test[text_field], tokenizer, model)\n",
    "            train_labels = np.array(train[\"label\"])\n",
    "            test_labels = np.array(test[\"label\"])\n",
    "\n",
    "            train_data[task] = [train_emb.tolist(), train_labels.tolist()]\n",
    "            test_data[task] = [test_emb.tolist(), test_labels.tolist()]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {task}: {e}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(f\"{output_prefix}_train_sets.json\", \"w\") as f:\n",
    "        json.dump(train_data, f)\n",
    "    with open(f\"{output_prefix}_test_sets.json\", \"w\") as f:\n",
    "        json.dump(test_data, f)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tasks to process\n",
    "classification_tasks = [\n",
    "    \"imdb\", \"ag_news\", \"yelp_polarity\", \"dbpedia_14\",\n",
    "    \"amazon_polarity\", \"emotion\", \"rotten_tomatoes\"\n",
    "]\n",
    "\n",
    "# Run for both models\n",
    "process_datasets(\"gpt2\", \"GPT2\", num_samples=1000)\n",
    "process_datasets(\"bert-base-uncased\", \"BERT\", num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f51c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mantisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
